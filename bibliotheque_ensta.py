# -*- coding: utf-8 -*-
"""bibliotheque_ensta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fshf_0gMpGJvCxMoB9dIVhjIoMRQxW9U
"""

import pandas as pd
import pickle
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt

#ML
import sklearn
from sklearn import ensemble
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_predict
#from sklearn.feature_selection import RFECV, SequentialFeatureSelector

#import catboost

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA

#DL
from keras.models import Sequential, Model
from keras.layers import Dense, Activation, Dropout, BatchNormalization, Input
from keras.callbacks import EarlyStopping
import tensorflow_probability as tfp
import tensorflow as tf
from tensorflow.keras.optimizers import Adam

from bibliotheque_ensta import *

def root_mean_squarred(x1,x2) :
    return (x1**2 + x2**2)**0.5

def visu_res_vent(pred, target):
    histo = pd.DataFrame(data={'pred':pred, 'target':target})

    plt.figure(1,figsize=(20,10))
    plt.subplot(2,1,1)
    sns.histplot(histo)
    plt.subplot(2,1,2)
    sns.lineplot(data = histo)

def convert_time(df, colonne) :
  """
  Ici, les paramètres de cette fonction sont : 
  df : dataframe qui contient les relevés
  colonne : string qui correspond au nom de la colonne qui contient les relevés dont on souhaite obtenir les informations temporelles
  """

  col_time = df[colonne]
  time = col_time.apply(lambda x: datetime.datetime.strptime(x,"%Y-%m-%dT%H:%M:%S.%fZ"))
  year = time.apply(lambda x : x.year)
  month = time.apply(lambda x : x.month)
  day = time.apply(lambda x : x.day)
  hour = time.apply(lambda x : x.hour)
  minute = time.apply(lambda x : x.minute)
  second = time.apply(lambda x : x.second)
  
  return year, month, day, hour, minute, second

def df_time(df, colonne) :
  """
  On utilise les mêmes paramètres que pour la fonction convert_time
  """
  year, month, day, hour, minute, second = convert_time(df, "timestamp")
  x = pd.DataFrame(data = {'year':year, 'month':month, 'day':day, 'hour':hour, 'minute':minute, 'second':second})
  col = df.columns.drop(colonne)
  return pd.concat([x, df[col]], axis = 1)

#Calculer au préalable y_pred_test et y_pred_train
def protocole_eval_vent(Y_train, Y_test, y_pred_train, y_pred_test, nom_methode):
  mae_test = metrics.mean_absolute_error(Y_test, y_pred_test)
  mae_train = metrics.mean_absolute_error(Y_train, y_pred_train)
  R2_test = metrics.r2_score(Y_test, y_pred_test)
  R2_train = metrics.r2_score(Y_train, y_pred_train)

  data = pd.DataFrame(data={"Train MAE":[mae_train], "Train R2":[R2_train], "Test MAE":[mae_test], "Test R2":[R2_test], "Résultat" : nom_methode})

  return data

def print_graph_vent(y_test, y_pred, title, path ='', save = False):
  affichage = pd.DataFrame(data = {"pred":y_pred, "target":y_test})
  x,y=[i for i in range (20)], [i for i in range(20)]

  sns.regplot(x = 'target', y = 'pred', data = affichage, scatter_kws = {'s': 50, 'color': 'blue'}, line_kws={'color':'red'})

  plt.plot(x,y,color='black')
  plt.grid()
  plt.title(title)
  #plt.xlabel("Actual wind speed (M/S)")
  plt.xlabel("Actual value")
  #plt.ylabel("Predicted wind speed (M/S)")
  plt.ylabel("Predicted value")
  if save == True :
    plt.savefig(path)

#Calculer au préalable y_pred_test et y_pred_train
def protocole_eval_classi_pluie(Y , y_pred, path='', save=False):
  cf_matrix = confusion_matrix(Y, y_pred)#, normalize = "all")
  print(sns.heatmap(cf_matrix, annot=True))
  if save == True :
    plt.savefig(path)

  #data = pd.DataFrame(data={"True Positive":[mae_train], "True Negative":[R2_train], "False Positive":[mae_test], "False Negative":[R2_test]})

  return cf_matrix[1][0]

#Calculer au préalable y_pred_test et y_pred_train
def protocole_eval_regr_pluie(Y_train, Y_test, y_pred_train, y_pred_test):
  mse_test = metrics.mean_squared_error(Y_test, y_pred_test)
  mse_train = metrics.mean_squared_error(Y_train, y_pred_train)

  data = pd.DataFrame(data={"Train MSE":[mse_train], "Test MSE":[mse_test]})

  return data
  
def build_affi_pred_rain(df1, pred_classi, pred_regr):
  #pas pluie
  pas_pluie = df1[(pred_classi == 0)][['year', 'month', 'day']]
  pas_pluie['pred'] = 0

  #pluie
  pluie_pred = df1[(pred_classi == 1)][['year', 'month', 'day', 'hour']]
  pluie_pred['pred'] = pred_regr

  #calcul moyennes
  group_pluie = pluie_pred.groupby(by=['year','month', 'day'])
  i=0
  moy_jour = []
  for key,item in group_pluie:
    a_group = group_pluie.get_group(key)
    moy_jour.append(np.sum(a_group['pred']))

  pluie_pred = df1[(pred_classi == 1)][['year', 'month', 'day']].drop_duplicates()
  pluie_pred['pred'] = pd.Series(moy_jour).values
  pluie_pred = pluie_pred.reset_index(drop=True)

  df_pluie = pd.concat([pas_pluie.drop_duplicates(), pluie_pred.drop_duplicates()])
  df_pluie = df_pluie.reset_index(drop=True)

  #colonne date
  date_ = []
  for i in range(len(df_pluie)):
    date_.append(str(df_pluie['year'].iloc[i]) + "-" + str(df_pluie['month'].iloc[i]) + "-" + str(df_pluie['day'].iloc[i]))

  df_pluie['date'] = pd.Series(date_)
  df_pluie['date'] = df_pluie['date'].apply(pd.to_datetime)

  #on classe par date
  df_pluie = df_pluie.sort_values(by=['date'], ascending=True)
  
  return df_pluie

  
def build_affi_target_rain(df1, target = 'rainfall_W1M3A'):
#target value maintenant
  group_pluie_target = df1.groupby(by=['year','month', 'day'])
  moy_target_jour = []
  for key,item in group_pluie_target:
    a_group = group_pluie_target.get_group(key)
    moy_target_jour.append(np.sum(a_group[target]))

  data_rain_affichage = df1[['year', 'month', 'day']].drop_duplicates()
  data_rain_affichage['target'] = moy_target_jour
  data_rain_affichage = data_rain_affichage.reset_index(drop=True)

  #colonne date
  date_bis = []
  for i in range(len(data_rain_affichage)):
    date_bis.append(str(data_rain_affichage['year'].iloc[i]) + "-" + str(data_rain_affichage['month'].iloc[i]) + "-" + str(data_rain_affichage['day'].iloc[i]))

  data_rain_affichage['date'] = pd.Series(date_bis)
  data_rain_affichage['date'] = data_rain_affichage['date'].apply(pd.to_datetime)

  return data_rain_affichage

def print_pluie(year1, year2, df1, df2, pred = 'pred', target = 'target', path='', save=False):
  p1 = plt.plot(df1[(df1['year']>=year1) & (df1['year'] <= year2)]['date'], df1[(df1['year']>=year1) & (df1['year'] <= year2)][pred], color="orange", label = 'Prédiction')
  p2 = plt.plot(df2[(df2['year']>=year1) & (df2['year'] <= year2)]['date'], df2[(df2['year']>=year1) & (df2['year'] <= year2)][target], color = 'blue', label = 'Target Value')
  plt.title("Affichage niveau de pluie")  
  plt.legend()
  plt.show()
  if save == True :
    plt.savefig(path)

def print_cumsum_pluie(year1, year2, df1, df2, pred = 'pred', target = 'target', path='', save=False):

  n1=len(df1[(df1['year']>=year1) & (df1['year'] <= year2)]['date'])
  values1 = df1[(df1['year']>=year1) & (df1['year'] <= year2)][pred]
  cumulative = np.cumsum(values1)
  p1 = plt.plot(df1[(df1['year']>=year1) & (df1['year'] <= year2)]['date'], cumulative, c='orange', label = 'Prédiction')

  n2=len(df2[(df2['year']>=year1) & (df2['year'] <= year2)]['date'])
  values2 = df2[(df2['year']>=year1) & (df2['year'] <= year2)][target]
  cumulative2 = np.cumsum(values2)
  p2 = plt.plot(df2[(df2['year']>=year1) & (df2['year'] <= year2)]['date'], cumulative2, c='blue', label = 'Target Value')
  
  plt.title('Cumulative rain histograms')
  plt.legend()
  plt.show()
  if save == True :
    plt.savefig(path)
    
def test_model_rf(nom_methode, X, Y, type_model = 'regression', path='', save=False):
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)

    if type_model == 'regression':
        regr = RandomForestRegressor()
        regr.fit(X_train, Y_train)
        y_pred_test = regr.predict(X_test)
        y_pred_train = regr.predict(X_train)

        eval_rf = protocole_eval_vent(Y_train, Y_test, y_pred_train, y_pred_test, nom_methode)
        print_graph_vent(Y_test, y_pred_test, "RandomForest Training", path, save)
        
        return regr, eval_rf, Y_test, y_pred_test
    
    if type_model == 'classification' :
        regr = RandomForestClassifier()
        regr.fit(X_train, Y_train)
        y_pred_rf = regr.predict(X)
        protocole_eval_classi_pluie(Y, y_pred_rf)
        return regr, y_pred_rf


def preparation_dl(X,Y):
    X_dl = X.values
    Y_dl = Y.values

    min_max_scaler = MinMaxScaler()
    X_dl = min_max_scaler.fit_transform(X_dl)

    X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_dl, Y_dl, test_size=0.3, shuffle = 10)
    X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)
    
    return X_train, Y_train, X_test, Y_test, X_val, Y_val

def definition_model(couches, parametre, X, type_model='linear'):
    model = Sequential()

    param = parametre
    for i in range(couches):    
        if i==0:
            model.add(Dense(param, input_shape = (X.shape[1],), activation = 'linear'))
            param = param *2
        else:
            model.add(Dense(param, activation = 'relu'))
            param = param *2
    
    model.add(Dropout(0.2))
    
    param = param/2
    
    for i in range(couches):    
        model.add(Dense(param, activation = 'relu'))
        param = param /2

    model.add(Dropout(0.2))
    
    if type_model == 'linear' :
        model.add(Dense(1, activation = 'linear'))
    else :
        model.add(Dense(2, activation = 'softmax'))
    
    return model

def affiche_hist(hist):
    plt.plot(hist.history['loss'])
    plt.plot(hist.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Val'], loc='upper right')
    plt.show()
    
def test_model_dl(nom_methode, couches, parametre, X, Y, epochs=25, batch_size=64, path='', save=False, type_model='linear'):
    X_train, Y_train, X_test, Y_test, X_val, Y_val = preparation_dl(X,Y)
    
    model = definition_model(couches, parametre, X_train, type_model)
    
    model.compile(loss='mean_squared_error',
              optimizer='adam',
              metrics=['accuracy'])
    
    hist = model.fit(X_train, Y_train,
          batch_size=batch_size, 
          epochs=epochs,
          validation_data=(X_val, Y_val))
    
    affiche_hist(hist)
    
    if type_model == 'linear' :
    
        y_pred_test = model.predict(X_test)
        y_pred_train = model.predict(X_train)

        y_pred = [float(y_pred_test[i]) for i in range(len(y_pred_test))]

        eval_nn = protocole_eval_vent(Y_train, Y_test, y_pred_train, y_pred_test, nom_methode)
    
        y_test = [float(Y_test[i]) for i in range(len(Y_test))]
    
        print_graph_vent(y_test, y_pred, "RandomForest Training", path, save)
    
        return eval_nn, model, hist
    
    else : 
        X_dl = X.values

        min_max_scaler = MinMaxScaler()
        X_dl = min_max_scaler.fit_transform(X_dl)

        pred_nn_faible = model.predict(X_dl)
        return pred_nn_faible, model, hist, model.evaluate(X_test, Y_test, verbose=0)
    

def prepa_total_tol_time(total_tol, total_time) :
    n = len(total_tol[0])
    df_total_tol = pd.DataFrame(data = {'total_tol' : [total_tol[i] for i in range(len(total_tol))]})
    def verif_nan(X) : 
        """Fonction qui vérifie si un tableau est rempli de nan"""
        return X.apply(lambda x : np.isnan(x).sum()==n)

    idx_nan_total_tol = df_total_tol[df_total_tol.apply(lambda x : verif_nan(x))==True].dropna().index.tolist()
    df_total_tol = df_total_tol.drop(idx_nan_total_tol, axis = 0).reset_index(drop=True)
    
    def remplace_inf(X):
        X[0][1:3] = 0
        X[0][4] = 0
        return X

    for i in range(len(df_total_tol)):
        remplace_inf(df_total_tol.loc[i])
    
    n = len(df_total_tol['total_tol'].loc[0])
    X = pd.DataFrame(columns = [i for i in range(n)])

    for i in range(len(df_total_tol)):
        X.loc[i] = df_total_tol['total_tol'].loc[i]
    
    hour = []
    day = []
    month = []
    year = []

    for i in range(len(total_time)):
        hour.append(total_time[i][0].hour)
        day.append(total_time[i][0].day)
        month.append(total_time[i][0].month)
        year.append(total_time[i][0].year)
        
    time = pd.DataFrame(data = {'year' : year, 'month' : month, 'day' : day, 'hour' : hour})
    time = time.drop(idx_nan_total_tol, axis = 0).reset_index(drop=True)
    
    X_time = pd.concat([time, X], axis = 1)
    
    return X_time

def model_poisson(couches, parametre, X):
    tfd = tfp.distributions

    inputs = Input(shape=(X.shape[1],)) 
    for i in range(couches):
        if i == 0:
            x = Dense(parametre, activation="relu")(inputs) 
            parametre = parametre/2
        else :
            x = Dense(int(parametre), activation="relu")(x) 
    
    rate = Dense(1, activation=tf.exp)(x) 
    p_y = tfp.layers.DistributionLambda(tfd.Poisson)(rate) 
    model_p = Model(inputs=inputs, outputs=p_y) 
    
    return model_p

def test_model_dl_poisson(nom_methode, couches, parametre, X, Y, epochs=25, batch_size=64, path='', save=False):
    X_train, Y_train, X_test, Y_test, X_val, Y_val = preparation_dl(X,Y)
    
    model = model_poisson(couches, parametre, X_train)
    

    def NLL(y_true, y_hat): 
      return -y_hat.log_prob(y_true)

    model.compile(Adam(learning_rate=0.01), loss=NLL)

    hist = model.fit(X_train, Y_train,
              batch_size=batch_size, epochs=epochs,
              validation_data=(X_val, Y_val))
    
    affiche_hist(hist)
    
    y_pred_test = model.predict(X_test)
    y_pred_train = model.predict(X_train)

    y_pred = [float(y_pred_test[i]) for i in range(len(y_pred_test))]

    eval_nn = protocole_eval_vent(Y_train, Y_test, y_pred_train, y_pred_test, nom_methode)
    
    y_test = [float(Y_test[i]) for i in range(len(Y_test))]
    
    print_graph_vent(y_test, y_pred, "RandomForest Training", path, save)
    
    return eval_nn, model, hist

def lance_dl(nom_methode, couches, parametre, X, Y, epochs=25, batch_size=64, type_model = 'normal', path='', save=False):
    if type_model == 'normal' :
        eval_nn, model, hist = test_model_dl(nom_methode, couches, parametre, X, Y, epochs, batch_size, path, save)
        return eval_nn, model, hist
    if type_model == 'poisson' :
        eval_nn, model, hist = test_model_dl_poisson(nom_methode, couches, parametre, X, Y, epochs, batch_size, path, save)
        return eval_nn, model, hist
    if type_model == 'classi':
        pred_nn_faible, model, hist, evaluate = test_model_dl(nom_methode, couches, parametre, X, Y, epochs, batch_size, path, save, type_model='classi')
        return eval_nn, model, hist

def donnees_regr_pluie(y_pred, X, Y) :
    Y_regr = Y[(y_pred == 1)]    
    X_regr = X[(y_pred == 1)]
    return X_regr, Y_regr 

def test_k_fold(n_fold, X, Y, couches, param, batch_size, epochs, prob='linear'):
  # Define the K-fold Cross Validator
  kfold = KFold(n_splits=n_fold, shuffle=True)

  acc_per_fold = []
  loss_per_fold = []
  i=1

  for train, test in kfold.split(X, Y):
    
      model = definition_model(couches, param, X, type_model=prob)
      model.compile(loss='mean_squared_error',
                optimizer='adam',
              metrics=['accuracy'])

      history = model.fit(X.loc[train], Y.loc[train],
                batch_size=batch_size,
                epochs=epochs,
                verbose=0)
    
      # Generate generalization metrics
      scores = model.evaluate(X.loc[test], Y.loc[test], verbose=0)
      acc_per_fold.append(scores[1] * 100)
      loss_per_fold.append(scores[0])
      print("["+">"*i+"-"*(n_fold-i)+"]")
      i+=1

  return pd.DataFrame(data={'loss':loss_per_fold}), pd.DataFrame(data={'acc':acc_per_fold})